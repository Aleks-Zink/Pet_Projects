{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLg1iVMUURRt"
      },
      "source": [
        "# Трансферное обучение в PyThorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbtKJJNDURRw"
      },
      "source": [
        "## Задача"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HshCSLjURRx"
      },
      "source": [
        "Cоздать свёрточную модель трансферного обучения для классификации бетона с трещиной (Positive mark) и бетона без трещины (Negative mark)  \n",
        "Как было показано в [1_Models_for_Transfer_Learning_PyTorch.ipynb](https://github.com/Aleks-Zink/Pet_Projects/blob/main/2_Concrete/1_Models_for_Transfer_Learning_PyTorch.ipynb), за основу будет взята модель VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3uhtZmxURRy"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R9hs59vURRy"
      },
      "source": [
        "40 000 цветных картинок бетона размером 227x227 пикселей, 20 000 из которых с стрещиной, другие 20 000 целый  \n",
        "Данные взяты из курса [AI Capstone Project with Deep Learning](https://www.coursera.org/learn/ai-deep-learning-capstone?specialization=ai-engineer), явлюющийся заключительным курсом [IBM AI Engineering Professional Certificate](https://www.coursera.org/professional-certificates/ai-engineer) на сайте [coursera.org](https://www.coursera.org/)  \n",
        "[Данные](https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHRFNOFTdOvS",
        "outputId": "e55838de-ba46-473c-c00d-8619512c465e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-04-28 21:22:42--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 245259777 (234M) [application/zip]\n",
            "Saving to: ‘concrete_crack_images_for_classification.zip’\n",
            "\n",
            "concrete_crack_imag 100%[===================>] 233.90M  26.8MB/s    in 8.3s    \n",
            "\n",
            "2023-04-28 21:22:51 (28.1 MB/s) - ‘concrete_crack_images_for_classification.zip’ saved [245259777/245259777]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "97PS2_9QdPYL"
      },
      "outputs": [],
      "source": [
        "!mkdir ./data\n",
        "!mkdir ./data/concrete\n",
        "!mkdir ./models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLNjIsLqdPhr"
      },
      "outputs": [],
      "source": [
        "!unzip concrete_crack_images_for_classification.zip -d ./data/concrete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgFcxv8gURRy"
      },
      "source": [
        "## Расчёты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pSiu-fxCURRz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GTpvmZiHURR0"
      },
      "outputs": [],
      "source": [
        "class Concrete(Dataset):\n",
        "    def __init__(self, train=True, train_size=0.75, transform=None):\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        # директории классов\n",
        "        directory = \"./data/concrete/\"\n",
        "        positive = \"Positive\"\n",
        "        negative = \"Negative\"\n",
        "\n",
        "        # путь к классам\n",
        "        positive_path = os.path.join(directory, positive)\n",
        "        negative_path = os.path.join(directory, negative)\n",
        "\n",
        "        # путь к каждому положительному объекту\n",
        "        positive_files = [os.path.join(positive_path, file) for file in os.listdir(positive_path) if file.endswith(\".jpg\")]\n",
        "        positive_files.sort()\n",
        "\n",
        "        # путь к каждому негативному объекту\n",
        "        negative_files = [os.path.join(negative_path, file) for file in os.listdir(negative_path) if file.endswith(\".jpg\")]\n",
        "        negative_files.sort()\n",
        "\n",
        "        # пути к каждому объекту (чётные индексы - положительные классы, нечётные индексы - отрицательные классы)\n",
        "        self.all_files = []\n",
        "        for pair in zip(positive_files, negative_files):\n",
        "            self.all_files.extend(pair)\n",
        "        \n",
        "        # всего объектов в данных\n",
        "        length = len(self.all_files)\n",
        "\n",
        "        # целевое обозначение каждого объекта \n",
        "        self.all_targets = torch.zeros(length, dtype=torch.long)\n",
        "        self.all_targets[::2] = 1\n",
        "\n",
        "        # индекс границы между данными обечения и данными валидации\n",
        "        border = int(length * train_size)\n",
        "\n",
        "        # разделение данных на тестовую валидационную выборки\n",
        "        if train:\n",
        "            self.all_files = self.all_files[:border]\n",
        "            self.all_targets = self.all_targets[:border]\n",
        "        else:\n",
        "            self.all_files = self.all_files[border:]\n",
        "            self.all_targets = self.all_targets[border:]\n",
        "        \n",
        "        self.len = len(self.all_targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        \n",
        "        # загрузка оъекта и его обозначения\n",
        "        image = Image.open(self.all_files[ind])\n",
        "        y = self.all_targets[ind]\n",
        "\n",
        "        # трансформация объекта, если это необходимо\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tJy4djxgURR1"
      },
      "outputs": [],
      "source": [
        "def train(model, data_train, data_val, val_len, device='cpu', epochs=3, lr=0.001, save_model=False, acc_max=0):\n",
        "    \n",
        "    # инициализация оптимизатора и функции ошибки\n",
        "    optimizer = optim.Adam(params=[parameter for parameter in model.parameters() if parameter.requires_grad], lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # перевод модель на устройство для расчётов\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        # перевод модели в режим обучения\n",
        "        model.train()\n",
        "\n",
        "        # цикл обучения\n",
        "        for x, y in tqdm(data_train, desc=f\"Train epoch {epoch + 1: >3}\"):\n",
        "\n",
        "            # перевод данных на устройсто для расчётов         \n",
        "            x, y = x.to(device), y.to(device)\n",
        "            \n",
        "            # обнуление градиента\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # расчёт ошибки\n",
        "            y_hat = model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "\n",
        "            # шаг обучения\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # перевод модели в режим оценки\n",
        "        model.eval()\n",
        "\n",
        "        # число корректных ответов\n",
        "        correct = 0\n",
        "\n",
        "        # цикл оценки\n",
        "        for x, y in tqdm(data_val, desc=f\"Val epoch {epoch + 1: >5}\"):\n",
        "\n",
        "            # перевод данных на устройсто для расчётов \n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # сбор корректных ответов в батче\n",
        "            with torch.no_grad():\n",
        "                y_hat = model(x)\n",
        "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
        "\n",
        "        # расчёт точности\n",
        "        accuracy = correct / val_len\n",
        "        \n",
        "        print(f\"\\nAccuracy: {accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "        # сохранение наилучшей модели\n",
        "        if save_model and (acc_max < accuracy):\n",
        "            acc_max = accuracy\n",
        "            torch.save(model, './models/VGG16_for_Concrete_PyTorch.pt')\n",
        "            print(\"=== Модель сохранена ===\\n\")\n",
        "\n",
        "    # перевод модели на процессор\n",
        "    model.to(\"cpu\")\n",
        "\n",
        "    return accuracy, acc_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DpRm09_URR2",
        "outputId": "0b7183aa-925f-45ce-efdf-0a71b76aae9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 76.6MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# веса модели VGG16\n",
        "weights = models.VGG16_Weights.IMAGENET1K_V1\n",
        "\n",
        "# инициализация модели\n",
        "model = models.vgg16(weights=weights)\n",
        "\n",
        "# заморозка весов свёрточной модели\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False \n",
        "\n",
        "# замена полносвязных слоёв в конце\n",
        "model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "model.classifier = nn.Linear(512, 2)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RupchicPURR3"
      },
      "outputs": [],
      "source": [
        "# размер выборки для обучения\n",
        "train_size = 0.75\n",
        "# преобразователь данных\n",
        "transform = weights.transforms()\n",
        "# размер пакета данных\n",
        "batch_size = 100\n",
        "# перемешивать данные\n",
        "shuffle = True\n",
        "\n",
        "\n",
        "# инициализация обучающих данных\n",
        "data_train = Concrete(train=True, train_size=train_size, transform=transform)\n",
        "# инициализация валидационных данных\n",
        "data_val = Concrete(train=False, train_size=train_size, transform=transform)\n",
        "\n",
        "\n",
        "# размер валидационной выборки\n",
        "val_len = len(data_val)\n",
        "\n",
        "\n",
        "# пакетированные данные для тренировки\n",
        "data_train = DataLoader(dataset=data_train, batch_size=batch_size, shuffle=shuffle)\n",
        "# пакетированные данные для валидации\n",
        "data_val = DataLoader(dataset=data_val, batch_size=batch_size, shuffle=shuffle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBEJSiuCURR3",
        "outputId": "9d972b2a-378e-4142-ce5f-94d973a4591b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch   1: 100%|██████████| 300/300 [02:47<00:00,  1.79it/s]\n",
            "Val epoch     1: 100%|██████████| 100/100 [01:31<00:00,  1.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 99.13%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch   2: 100%|██████████| 300/300 [02:47<00:00,  1.79it/s]\n",
            "Val epoch     2: 100%|██████████| 100/100 [01:31<00:00,  1.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 99.25%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch   3: 100%|██████████| 300/300 [02:47<00:00,  1.79it/s]\n",
            "Val epoch     3: 100%|██████████| 100/100 [01:31<00:00,  1.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 99.36%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# обучение модели\n",
        "accuracy, acc_max = train(model,\n",
        "                          data_train=data_train,\n",
        "                          data_val=data_val,\n",
        "                          val_len=val_len,\n",
        "                          device=device,\n",
        "                          epochs=3,\n",
        "                          lr=1e-3,\n",
        "                          save_model=False,\n",
        "                          acc_max=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgOWfEhMURR3"
      },
      "source": [
        "Для повышения точности разморозим все веса модели и дообучим всю модель с уменьшенной скоростью, для подстраивания свёрточных слоёв под наши данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7YVmQCNURR4",
        "outputId": "cd90a16b-062b-4440-fc6f-6951ff759d53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch   1: 100%|██████████| 300/300 [07:16<00:00,  1.45s/it]\n",
            "Val epoch     1: 100%|██████████| 100/100 [01:30<00:00,  1.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 99.88%\n",
            "\n",
            "=== Модель сохранена ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch   2: 100%|██████████| 300/300 [07:16<00:00,  1.46s/it]\n",
            "Val epoch     2: 100%|██████████| 100/100 [01:34<00:00,  1.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 99.54%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch   3: 100%|██████████| 300/300 [07:14<00:00,  1.45s/it]\n",
            "Val epoch     3: 100%|██████████| 100/100 [01:31<00:00,  1.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 99.85%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# разморозка весов\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# обучение модели c уменьшенной скоростью обучения\n",
        "accuracy, acc_max = train(model,\n",
        "                          data_train=data_train,\n",
        "                          data_val=data_val,\n",
        "                          val_len=val_len,\n",
        "                          device=device,\n",
        "                          epochs=3,\n",
        "                          lr=1e-4,\n",
        "                          save_model=True,\n",
        "                          acc_max=accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9Dao8KFURR4",
        "outputId": "0942ba29-36d4-4ba9-9e53-c61dd1323995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Получаем модель c:\n",
            "\tОбъёмом 56.14 Мб\n",
            "\tТочностью 99.88%\n"
          ]
        }
      ],
      "source": [
        "# загружаем модель с наилучшей точностью\n",
        "model = torch.load('./models/VGG16_for_Concrete_PyTorch.pt')\n",
        "\n",
        "# вычисление числа параметров модели\n",
        "number_of_param = 0\n",
        "for param in model.parameters():\n",
        "    number_of_param += param.numel()\n",
        "\n",
        "print(f\"Получаем модель c:\\n\\tОбъёмом {number_of_param * 4 / (1024 ** 2):.2f} Мб\\n\\tТочностью {acc_max * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cGB4c6eURR4"
      },
      "source": [
        "Для дальнейшего использования модели, входные данные должны иметь форму (B, C, H, W)  \n",
        "Где:\n",
        " - B: количество фото для анализа\n",
        " - С = 3: цветовых каналов\n",
        " - H = 224: высота фото\n",
        " - W = 224: ширина фото"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avYBvMQYURR4"
      },
      "source": [
        "## Результаты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cURI8h7dNre"
      },
      "source": [
        "Была получена свёрточная модель трансферного обучения на базе модели VGG16 с размером модели в ~56 Мб.  \n",
        "Получившаяся модель с точностью ~99.88% может классифицировать фотографии бетона с трещиной и бетона без трещины"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpOpsCB1dNre"
      },
      "source": [
        "## Опыт работы в PyTorch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ITMeR7ZMdNrf"
      },
      "source": [
        "Плюсы:\n",
        " + Простая установка  \n",
        " + Простота освоения\n",
        " + Минимализм\n",
        " \n",
        "Минусы:\n",
        " + Большинство вещей делается вручную (к примеру: цикл обучения, метрики для оценки моделей)\n",
        "\n",
        "Лично для себя, на данный момент, выбираю PyTorch в качестве основной бибилиотеки для NN, так как больше опыта в её использовании"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
